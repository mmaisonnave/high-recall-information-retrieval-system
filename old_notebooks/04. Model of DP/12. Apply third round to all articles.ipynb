{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63365fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import info, ok, warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65f47e",
   "metadata": {},
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb31eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: ['1411697642', '1143160388', '1238204962', '2459964104', '2459666609', '1222379804', '1242257052', '1239753620', '1151348424', '2122281371', '1238440920', '1136691129', '2122279956']\n",
      "2022-03-17 17:21:36.735657 [ \u001b[1;94mINFO\u001b[0m  ] len(relevant_set)   = 581\n",
      "2022-03-17 17:21:36.735831 [ \u001b[1;94mINFO\u001b[0m  ] len(irrelevant_set) = 6523\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from utils.general import id2file\n",
    "\n",
    "second_round_data = [linea.strip().split(';') for linea in  open('second_roud_labels.csv','r').read().splitlines()]\n",
    "irrelevant_set = set([id_ for id_,label in second_round_data if label=='I'])\n",
    "relevant_set = set([id_ for id_,label in second_round_data if label=='R'])\n",
    "\n",
    "\n",
    "# Loading new_data\n",
    "new_data = [line.split(';') for line in open('new_data.csv').read().splitlines()]\n",
    "relevant_set = relevant_set.union(set([id_ for id_,label in new_data if label.strip()=='R']))\n",
    "irrelevant_set = irrelevant_set.union(set([id_ for id_,label in new_data if label.strip()=='I']))\n",
    "\n",
    "# Loading original data\n",
    "DP_examples_dirpath = '/home/ec2-user/SageMaker/mariano/notebooks/04. Model of DP/DP-relevant articles/'\n",
    "\n",
    "first_data = []\n",
    "for dirpath, dirnames, filenames in os.walk(DP_examples_dirpath):\n",
    "    for filename in filenames:\n",
    "        content = open(os.path.join(dirpath,filename),'r').read()\n",
    "        ids = re.findall('/docview/([^/]*)/',content)\n",
    "        relevant_set = relevant_set.union(set(ids))\n",
    "    \n",
    "# articles containg DP and Canada from that period, that were not deteted by Serperi\n",
    "GM_dp_dirpath = '/home/ec2-user/SageMaker/data/GM_DP_and_Canada1945_1967/'\n",
    "\n",
    "files = os.listdir(GM_dp_dirpath)\n",
    "\n",
    "irrelevant_set = irrelevant_set.union([file_[:-4] for file_ in files if file_[:-4] not in relevant_set and file_.endswith('.xml')])\n",
    "\n",
    "not_found=[]\n",
    "for id_ in list(relevant_set)+list(irrelevant_set):\n",
    "    if id2file(id_) is None:\n",
    "        not_found.append(id_)\n",
    "print(f'Not found: {not_found}')\n",
    "for id_ in not_found:\n",
    "    relevant_set = relevant_set.difference(set(not_found))\n",
    "    irrelevant_set = irrelevant_set.difference(set(not_found))\n",
    "    \n",
    "info(f'len(relevant_set)   = {len(relevant_set)}')\n",
    "info(f'len(irrelevant_set) = {len(irrelevant_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d392dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.zeros(shape=(len(relevant_set)+len(irrelevant_set),))\n",
    "y[:len(relevant_set)]=1\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbcd5a",
   "metadata": {},
   "source": [
    "### Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c64951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from utils.models import glove300_vectorize, glove600_vectorize\n",
    "from utils.tdmstudio import TDMStudio\n",
    "\n",
    "titles, texts = [], []\n",
    "for id_ in list(relevant_set)+list(irrelevant_set):\n",
    "    title, text = TDMStudio.get_title_and_text(id2file(id_))\n",
    "    titles.append(title)\n",
    "    texts.append(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b391d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 1s, sys: 3min 55s, total: 33min 57s\n",
      "Wall time: 34min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "data.append(glove300_vectorize(titles,texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904c5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.append(glove600_vectorize(titles,texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656ec432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.models import tokenize\n",
    "from utils.tdmstudio import TDMStudio\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "invalid = set([sw for sw in stopwords if any([token for token in tokenize(sw) if not token in stopwords ])]) # ['‘ve', \"'m\", '’ve', \"'ve\", '’m', '‘m', '‘d', '‘ll']\n",
    "stopwords = set(stopwords.difference(invalid)) \n",
    "vectorizer = TfidfVectorizer(\n",
    "                             input='content',\n",
    "                             lowercase=True,\n",
    "                             preprocessor=None,\n",
    "                             tokenizer=tokenize,\n",
    "                             analyzer='word',\n",
    "                             stop_words=list(stopwords),\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b\", #selects tokens of 2 or more alphanumeric char (punctuation is completely ignored and treated as token separator)\n",
    "                                                             # UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None\n",
    "    \n",
    "                             ngram_range=(1,1), #lower and upper boundary of the range of n-values for different n-grams.\n",
    "                             max_df=1.0, #ignore terms that have a document frequency strictly higher than given threshold\n",
    "                             min_df=0.001, #ignore terms that have a document frequency strictly lower than given threshold\n",
    "                             max_features=50000, #build a vocabulary that only considers the top max_features ordered by term frequency acoss the corpus\n",
    "                             vocabulary=None, #vocabulary is determined from the input documents\n",
    "                             norm='l2',\n",
    "                             use_idf=False,\n",
    "                             )\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return ''.join([char for char in word if not char in string.punctuation+' '])\n",
    "\n",
    "def tokenize(str_):\n",
    "    tokens = [word.lemma_.lower() for word in nlp(str_) if not word.is_stop]\n",
    "    tokens = [word.replace('\\n', '') for word in tokens if not word.isnumeric() and len(remove_punctuation(word))!=0]\n",
    "    return tokens\n",
    "\n",
    "data.append(vectorizer.fit_transform([f'{title}. {text}' for title, text in zip(titles,texts)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b12dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab8d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "          SVC(C=7, kernel='rbf', probability=True),             # GloVe300\n",
    "          SVC(C=1, degree=3, kernel='poly', probability=True),  # GloVe600\n",
    "          SVC(C=1, kernel='linear', probability=True),          # BOW + TF\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980716e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 178 ms, total: 3min 8s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for X,model in zip(data,models):\n",
    "    model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296535ce",
   "metadata": {},
   "source": [
    "### Applying to unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a0975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 12:42:27.356061 [ \u001b[1;94mINFO\u001b[0m  ] Number of possible relevant articles: 4,973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/GM_all_1945_1956/1323603426.xml'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.general import info,ok,warning,id2file\n",
    "import os\n",
    "predictions_dirpath = './predictions/'\n",
    "\n",
    "possible_relevant = [id2file(f[:-5]) for f in os.listdir(predictions_dirpath) if f.endswith('_v3.p')]\n",
    "info(f'Number of possible relevant articles: {len(possible_relevant):,}')\n",
    "assert all([not elem is None for elem in possible_relevant])\n",
    "assert all([type(elem)==str for elem in possible_relevant])\n",
    "possible_relevant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7d8854d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3546/3875504694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m best_paragraph(\n\u001b[0m\u001b[1;32m      6\u001b[0m                \u001b[0;34m'/home/ec2-user/SageMaker/data/GM_all_1945_1956/1294154032.xml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;34m[\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0m_glove300_vectorize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/mariano/notebooks/04. Model of DP/utils/models.py\u001b[0m in \u001b[0;36mbest_paragraph\u001b[0;34m(file_, vectorizers, models)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;31m#     for dtitle,dtext in zip(nlp.pipe(titles),nlp.pipe(texts)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.models import best_paragraph\n",
    "\n",
    "import os\n",
    "\n",
    "best_paragraph(\n",
    "               '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1294154032.xml',\n",
    "               [vectorizer.transform ,_glove300_vectorize],\n",
    "               [models[2], models[0]]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d9fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e705619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1294154032_v3.p'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in os.listdir('predictions') if f.endswith('_v3.p')][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cad39d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/GM_all_1945_1956/1294154032.xml'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2file('1294154032')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa973c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
