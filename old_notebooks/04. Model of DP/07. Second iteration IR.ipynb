{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453c5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import info, ok, warning, id2file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1cc5c",
   "metadata": {},
   "source": [
    "### Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2315678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: ['2122279956', '1151348424', '1242257052', '1136691129', '1411697642', '1238204962', '1143160388', '2459666609', '1222379804', '1239753620', '1238440920', '2122281371', '2459964104']\n",
      "2022-03-10 00:39:57.013015 [ \u001b[1;94mINFO\u001b[0m  ] len(relevant_set)   = 542\n",
      "2022-03-10 00:39:57.013161 [ \u001b[1;94mINFO\u001b[0m  ] len(irrelevant_set) = 6478\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "relevant_set = set()\n",
    "irrelevant_set = set()\n",
    "\n",
    "# Loading new_data\n",
    "new_data = [line.split(';') for line in open('new_data.csv').read().splitlines()]\n",
    "relevant_set = relevant_set.union(set([id_ for id_,label in new_data if label.strip()=='R']))\n",
    "irrelevant_set = irrelevant_set.union(set([id_ for id_,label in new_data if label.strip()=='I']))\n",
    "\n",
    "# Loading original data\n",
    "DP_examples_dirpath = '/home/ec2-user/SageMaker/mariano/notebooks/04. Model of DP/DP-relevant articles/'\n",
    "\n",
    "first_data = []\n",
    "for dirpath, dirnames, filenames in os.walk(DP_examples_dirpath):\n",
    "    for filename in filenames:\n",
    "        content = open(os.path.join(dirpath,filename),'r').read()\n",
    "        ids = re.findall('/docview/([^/]*)/',content)\n",
    "        relevant_set = relevant_set.union(set(ids))\n",
    "    \n",
    "# articles containg DP and Canada from that period, that were not deteted by Serperi\n",
    "GM_dp_dirpath = '/home/ec2-user/SageMaker/data/GM_DP_and_Canada1945_1967/'\n",
    "\n",
    "files = os.listdir(GM_dp_dirpath)\n",
    "\n",
    "irrelevant_set = irrelevant_set.union([file_[:-4] for file_ in files if file_[:-4] not in relevant_set and file_.endswith('.xml')])\n",
    "\n",
    "not_found=[]\n",
    "for id_ in list(relevant_set)+list(irrelevant_set):\n",
    "    if id2file(id_) is None:\n",
    "        not_found.append(id_)\n",
    "print(f'Not found: {not_found}')\n",
    "for id_ in not_found:\n",
    "    relevant_set = relevant_set.difference(set(not_found))\n",
    "    irrelevant_set = irrelevant_set.difference(set(not_found))\n",
    "    \n",
    "info(f'len(relevant_set)   = {len(relevant_set)}')\n",
    "info(f'len(irrelevant_set) = {len(irrelevant_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14991396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-10 00:40:12.362435 [ \u001b[1;94mINFO\u001b[0m  ] Creating models...\n",
      "2022-03-10 00:40:29.889101 [ \u001b[1;94mINFO\u001b[0m  ] Loading data\n",
      "2022-03-10 00:40:29.890839 [ \u001b[1;94mINFO\u001b[0m  ] Data loaded from disk.\n",
      "2022-03-10 00:40:31.175349 [ \u001b[1;94mINFO\u001b[0m  ] Training models\n",
      "2022-03-10 00:43:11.307201 [  \u001b[1;92mOK\u001b[0m   ] Done!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from utils.models import get_glove600,get_glove300, get_bow\n",
    "from utils.general import id2file \n",
    "import numpy as np\n",
    "from utils.tdmstudio import TDMStudio\n",
    "\n",
    "import pickle\n",
    "\n",
    "info('Creating models...')\n",
    "classifiers = [\n",
    "               SVC(kernel='linear', probability=True),              # BOW\n",
    "               SVC(C=15, kernel='linear', probability=True),        # GloVe 300\n",
    "               SVC(C=4, degree=1, kernel='poly', probability=True) # GloVe 600\n",
    "              ]\n",
    "files = [id2file(id_) for id_ in list(relevant_set)+list(irrelevant_set)]\n",
    "\n",
    "\n",
    "texts_and_titles = [TDMStudio.get_title_and_text(file_) for file_ in files]\n",
    "info('Loading data')\n",
    "if os.path.isfile('cache/data.p'):\n",
    "    info('Data loaded from disk.')\n",
    "    data = pickle.load(open('cache/data.p','rb'))\n",
    "else:\n",
    "    warning('Data needs to be rebuild.')\n",
    "    data = [\n",
    "            np.vstack([get_bow(title, text) for title, text in tqdm(texts_and_titles)]),\n",
    "            np.vstack([get_glove300(title, text) for title, text in tqdm(texts_and_titles)]),\n",
    "            np.vstack([get_glove600(title, text) for title, text in tqdm(texts_and_titles)]),\n",
    "#             np.vstack = stack([get_glove600(id_) for id_ in tqdm(files)]),\n",
    "           ]\n",
    "\n",
    "y = np.zeros(shape=(len(files)))\n",
    "y[:len(relevant_set)]=1\n",
    "\n",
    "info('Training models')\n",
    "for model,X in zip(classifiers,data):\n",
    "    model.fit(X,y)\n",
    "ok('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "rtas = [\n",
    "       cross_validate(classifiers[0],X,y,scoring=['accuracy','precision','recall','f1']),\n",
    "       cross_validate(classifiers[1],X,y,scoring=['accuracy','precision','recall','f1']),\n",
    "       cross_validate(classifiers[2],X,y,scoring=['accuracy','precision','recall','f1']),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfa7ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages\n",
      "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1\n",
      "0  5.427517    0.128385       0.966952        0.786546      0.78595  0.785293\n",
      "\n",
      "Averages\n",
      "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1\n",
      "0  5.512406     0.10344       0.962393         0.75481     0.761961  0.756796\n",
      "\n",
      "Averages\n",
      "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1\n",
      "0  6.063117    0.149544       0.967664        0.812515     0.758274  0.783145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for rta in rtas:\n",
    "#     print(pd.DataFrame(rta))\n",
    "    print(\"Averages\")\n",
    "    print(pd.DataFrame(np.average(pd.DataFrame(rta).values,axis=0).reshape(1,6),columns=pd.DataFrame(rta).columns))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a282b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-10 00:43:20.792650 [  \u001b[1;92mOK\u001b[0m   ] done\n"
     ]
    }
   ],
   "source": [
    "GM_all_part1 = '/home/ec2-user/SageMaker/data/GM_all_1945_1956/'\n",
    "GM_all_part2 = '/home/ec2-user/SageMaker/data/GM_all_1957-1967/'\n",
    "\n",
    "files = [GM_all_part1+filename for filename in os.listdir(GM_all_part1)]\n",
    "files += [GM_all_part2+filename for filename in os.listdir(GM_all_part2)]\n",
    "\n",
    "predictions_dirpath = './predictions/'\n",
    "\n",
    "\n",
    "def process_file(file_):\n",
    "    file_id = file_.split('/')[-1][:-4]\n",
    "    output_file = predictions_dirpath+file_id+'_v2.p'\n",
    "    if not os.path.isfile(output_file):\n",
    "        yhat = np.zeros(shape=(3,))\n",
    "        title, text = TDMStudio.get_title_and_text(file_)\n",
    "        x_bow = get_bow(title, text)\n",
    "        x_glove300 = get_glove300(title, text)  \n",
    "        x_glove600 = get_glove600(title, text)\n",
    "        for idx,(model,x) in enumerate(zip(classifiers, [x_bow,x_glove300, x_glove600])):\n",
    "            yhat[idx]=model.predict_proba([x])[0,1]\n",
    "        pickle.dump(yhat,open(output_file, 'wb'))\n",
    "        del(x_bow,x_glove300,x_glove600,title,text,yhat,file_id,output_file)\n",
    "ok('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9753bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-10 00:51:49.987474 [ \u001b[1;94mINFO\u001b[0m  ] Starting...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "writer = open('done.txt', 'w')\n",
    "writer.write(f'{datetime.datetime.now()} Starting...\\n')\n",
    "\n",
    "info('Starting...')\n",
    "\n",
    "files = files[1067422:]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    executor.map(process_file, files, chunksize=1000)\n",
    "\n",
    "writer.write(f'{datetime.datetime.now()} Done!')\n",
    "writer.close()\n",
    "ok('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fd6b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello worlds\n"
     ]
    }
   ],
   "source": [
    "print('hello worlds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc4697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
