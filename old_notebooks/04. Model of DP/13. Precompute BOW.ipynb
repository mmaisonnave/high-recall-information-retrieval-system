{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74a654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:00:54.066413 [ \u001b[1;94mINFO\u001b[0m  ] Cantidad de archivos 2,057,868\n",
      "2022-03-26 21:00:54.685888 [ \u001b[1;94mINFO\u001b[0m  ] Cantidad de archivos 1,000,000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.general import info,ok, warning\n",
    "import os\n",
    "\n",
    "GM1 = '/home/ec2-user/SageMaker/data/GM_all_1945_1956/'\n",
    "GM2 = '/home/ec2-user/SageMaker/data/GM_all_1957-1967/'\n",
    "\n",
    "files = [GM1+f for f in os.listdir(GM1)]+[GM2+f for f in os.listdir(GM2)]\n",
    "info(f'Cantidad de archivos {len(files):,}')\n",
    "\n",
    "rndg = np.random.default_rng(2022)\n",
    "# print(files[:10])\n",
    "rndg.shuffle(files)\n",
    "# print(files[:10])\n",
    "\n",
    "files = files[:1000000]\n",
    "info(f'Cantidad de archivos {len(files):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fc309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7b3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 14:54:05.759820 [\u001b[1;31mWARNING\u001b[0m] Computing labeled data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114492"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# warning('Computing labeled data')\n",
    "# #########################\n",
    "# # NOT MENTIONING CANADA #\n",
    "# #########################\n",
    "# IN_CANADA_FILE = '../05. High Recall V2/auxiliary_notebooks/in_canada.csv'\n",
    "# assert os.path.isfile(IN_CANADA_FILE)\n",
    "\n",
    "# lines = open(IN_CANADA_FILE,'r').read().splitlines()\n",
    "# ids_set = set([line.split(';')[0] for line in lines if line.endswith('False')])\n",
    "# # labeled_data = [DataItem(id_) for id_ in irrelevant_ids]\n",
    "\n",
    "\n",
    "# #############################\n",
    "# # LABELED OVER THREE ROUNDS #\n",
    "# #############################\n",
    "# for line in open('../05. High Recall V2/labeled_data.csv').read().splitlines()[1:]:\n",
    "#     id_,label = line.split(';')\n",
    "#     ids_set.add(id_)\n",
    "# len(ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4e1a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/data/GM_all_1957-1967/1288562627.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1957-1967/1288387755.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1957-1967/1323532462.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1289625759.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1287366275.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1289454772.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1957-1967/1282877585.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1957-1967/1270333241.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1291769709.xml',\n",
       " '/home/ec2-user/SageMaker/data/GM_all_1945_1956/1323603473.xml']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from utils.general import id2file\n",
    "# files = [id2file(id_) for id_ in ids_set]\n",
    "# files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430e3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 721 ms, total: 4.7 s\n",
      "Wall time: 9.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from utils.tdmstudio import TDMStudio\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.models import tokenize\n",
    "# AUXs\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "\n",
    "# def remove_punctuation(word):\n",
    "#     return ''.join([char for char in word if not char in string.punctuation+' '])\n",
    "\n",
    "# def tokenize(str_):\n",
    "#     tokens = [word.lemma_.lower() for word in nlp(str_) if not word.is_stop and word.lemma_.isalnum()]\n",
    "#     tokens = [word.replace('\\n', '') for word in tokens if not word.isnumeric() and len(remove_punctuation(word.replace('\\n', '')))!=0]\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "from threading import Lock\n",
    "import gc\n",
    "\n",
    "def process_file(idx, file_):\n",
    "    title, text = TDMStudio.get_title_and_text(file_)\n",
    "    id_ = file_.split('/')[-1][:-4]\n",
    "    # Write\n",
    "#     wlock.acquire()\n",
    "    writer0.write(f'{id_}\\n')\n",
    "    writer0.flush()\n",
    "#     wlock.release()\n",
    "    \n",
    "    if not title is None and not text is None:        \n",
    "        tokens = tokenize(f'{title}. {text}') #tokenize(title) + tokenize(text)\n",
    "    \n",
    "        tokens_set = set(tokens)\n",
    "\n",
    "        ############################################\n",
    "        # UPDATE freq (remove low frequency terms) #\n",
    "        ############################################\n",
    "        if (idx+1)%25000==0:\n",
    "            to_remove = [word for word in freq if freq[word]<25]\n",
    "            info(f'Ite. no. {idx:10,} - No. of words to remove {len(to_remove):12,} - Remaining {len(freq)-len(to_remove)}')\n",
    "            for word in to_remove:\n",
    "                del(freq[word])\n",
    "            while len(to_remove)>0:\n",
    "                del(to_remove[0])\n",
    "            del(to_remove)\n",
    "            gc.collect()\n",
    "        for token in tokens_set:\n",
    "            freq[token]+=1\n",
    "\n",
    "        del(tokens_set,tokens)\n",
    "    del(title,text, id_)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c14ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586746 done_vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l done_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b1b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461504ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:01:32.917810 [ \u001b[1;94mINFO\u001b[0m  ] Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24995it [09:38, 37.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:11:12.208105 [ \u001b[1;94mINFO\u001b[0m  ] Ite. no.     24,999 - No. of words to remove      711,425 - Remaining 22758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49997it [20:28, 60.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:22:01.356605 [ \u001b[1;94mINFO\u001b[0m  ] Ite. no.     49,999 - No. of words to remove      730,758 - Remaining 24914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73123it [30:34, 43.09it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from utils.general import info, ok\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "writer0 = open('done_vocab.txt', 'w')\n",
    "\n",
    "info('Starting...')\n",
    "freq = defaultdict(int)\n",
    "\n",
    "for idx, file_ in tqdm(enumerate(files)):\n",
    "    process_file(idx,file_)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "#     executor.map(process_file, files, chunksize=4000)\n",
    "    \n",
    "info('Saving freq.txt')\n",
    "writer = open('precomputed/freq.txt', 'w')\n",
    "for word in freq:\n",
    "    if freq[word]>205:\n",
    "        writer.write(f'{word};{freq[word]}\\n')\n",
    "writer.close()\n",
    "\n",
    "# info('Saving vocab.txt')\n",
    "# writer = open('precomputed/vocab.txt', 'w')\n",
    "# for word in freq:\n",
    "#     if freq[word]>205:\n",
    "#         writer.write(f'{word};{freq[word]}\\n')\n",
    "# writer.close()\n",
    "\n",
    "\n",
    "writer0.close()\n",
    "ok('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b28d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 24 11:57:11 UTC 2022\r\n"
     ]
    }
   ],
   "source": [
    "!date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69beb618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 14K Mar 24 21:43 precomputed/freq.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 108K Mar 24 21:43 done_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh precomputed/freq.txt\n",
    "!ls -lh done_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0a6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441 precomputed/freq.txt\n",
      "10000 done_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l precomputed/freq.txt\n",
    "!wc -l done_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fab105",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(files)*(0.01/100.0))\n",
    "info(f'threshold={threshold}')\n",
    "vocab = [word for word in freq if freq[word]>threshold]\n",
    "info(f'len(vocab)={len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65079036",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e77485ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2057868/2057868 [57:24<00:00, 597.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.tdmstudio import TDMStudio\n",
    "from tqdm import tqdm\n",
    "corpus=[]\n",
    "for file_ in tqdm(files):\n",
    "    title, text = TDMStudio.get_title_and_text(file_)\n",
    "    corpus.append(f'{title}. {text}')\n",
    "    del(title,text)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.models import tokenize\n",
    "from utils.tdmstudio import TDMStudio\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# AUXs\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return ''.join([char for char in word if not char in string.punctuation+' '])\n",
    "\n",
    "def tokenize(str_):\n",
    "    tokens = [word.lemma_.lower() for word in nlp(str_) if not word.is_stop]\n",
    "    tokens = [word.replace('\\n', '') for word in tokens if not word.isnumeric() and len(remove_punctuation(word))!=0]\n",
    "    return tokens\n",
    "\n",
    "# STOPWORDS\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "invalid = set([sw for sw in stopwords if any([token for token in tokenize(sw) if not token in stopwords ])]) # ['‘ve', \"'m\", '’ve', \"'ve\", '’m', '‘m', '‘d', '‘ll']\n",
    "stopwords = set(stopwords.difference(invalid))\n",
    "\n",
    "#MODEL\n",
    "vectorizer = TfidfVectorizer(\n",
    "                             input='content',\n",
    "                             lowercase=True,\n",
    "                             preprocessor=None,\n",
    "                             tokenizer=tokenize,\n",
    "                             analyzer='word',\n",
    "                             stop_words=list(stopwords),\n",
    "#                              token_pattern=r\"(?u)\\b\\w\\w+\\b\", #selects tokens of 2 or more alphanumeric char (punctuation is completely ignored and treated as token separator)\n",
    "#                                                              # UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None\n",
    "    \n",
    "                             ngram_range=(1,1), #lower and upper boundary of the range of n-values for different n-grams.\n",
    "                             max_df=1.0, #ignore terms that have a document frequency strictly higher than given threshold\n",
    "                             min_df=0.001, #ignore terms that have a document frequency strictly lower than given threshold\n",
    "                             max_features=50000, #build a vocabulary that only considers the top max_features ordered by term frequency acoss the corpus\n",
    "                             vocabulary=None, #vocabulary is determined from the input documents\n",
    "                             norm='l2',\n",
    "                             use_idf=False,\n",
    "                             )\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823048be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2057868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gm.union(dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac3cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.models import tokenize\n",
    "from utils.tdmstudio import TDMStudio\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# AUXs\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return ''.join([char for char in word if not char in string.punctuation+' '])\n",
    "\n",
    "def tokenize(str_):\n",
    "    tokens = [word.lemma_.lower() for word in nlp(str_) if not word.is_stop]\n",
    "    tokens = [word.replace('\\n', '') for word in tokens if not word.isnumeric() and len(remove_punctuation(word))!=0]\n",
    "    return tokens\n",
    "\n",
    "# STOPWORDS\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "invalid = set([sw for sw in stopwords if any([token for token in tokenize(sw) if not token in stopwords ])]) # ['‘ve', \"'m\", '’ve', \"'ve\", '’m', '‘m', '‘d', '‘ll']\n",
    "stopwords = set(stopwords.difference(invalid))\n",
    "\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f75a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
