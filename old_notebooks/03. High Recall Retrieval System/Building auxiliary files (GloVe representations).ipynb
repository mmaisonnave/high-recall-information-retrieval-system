{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014c133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe764340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Globe and Mail articles: 151,004\n",
      "['1041435397.xml', '1124924513.xml', '1124924549.xml', '1124928180.xml', '1124937415.xml']\n",
      "['2610545245.xml', '2611600803.xml', '2611601132.xml', '2611601516.xml', '2611601526.xml']\n",
      "\n",
      "Number of Toronto Star articles: 41,429\n",
      "['1000282449.xml', '1000282615.xml', '1000282859.xml', '1000282969.xml', '1000345426.xml']\n",
      "['994013461.xml', '994013464.xml', '994013511.xml', '994014287.xml', '994377993.xml']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GM_datapath = '/home/ec2-user/SageMaker/data/The_Globe_and_Mail_with_DP_filter_by_article_type/'\n",
    "TS_datapath = '/home/ec2-user/SageMaker/data/Toronto_Star_Publication_with_query/'\n",
    "GloVe_datapath ='/home/ec2-user/SageMaker/mariano/notebooks/03. High Recall Retrieval System/vectors/'\n",
    "GM_filenames = os.listdir(GM_datapath)\n",
    "print(f\"Number of Globe and Mail articles: {len(GM_filenames):,}\" )\n",
    "GM_filenames.sort()\n",
    "print(GM_filenames[:5])\n",
    "print(GM_filenames[-5:])\n",
    "\n",
    "# Number of Globe and Mail articles: 151,004\n",
    "# ['1041435397.xml', '1124924513.xml', '1124924549.xml', '1124928180.xml', '1124937415.xml']\n",
    "# ['2610545245.xml', '2611600803.xml', '2611601132.xml', '2611601516.xml', '2611601526.xml']\n",
    "\n",
    "print()\n",
    "TS_filenames = os.listdir(TS_datapath)\n",
    "print(f\"Number of Toronto Star articles: {len(TS_filenames):,}\" )\n",
    "TS_filenames.sort()\n",
    "print(TS_filenames[:5])\n",
    "print(TS_filenames[-5:])\n",
    "# Number of Toronto Star articles: 41,429\n",
    "# ['1000282449.xml', '1000282615.xml', '1000282859.xml', '1000282969.xml', '1000345426.xml']\n",
    "# ['994013461.xml', '994013464.xml', '994013511.xml', '994014287.xml', '994377993.xml']\n",
    "\n",
    "# There are no two articles with same ID (and filename)\n",
    "assert len(TS_filenames)+len(GM_filenames) == len(set(TS_filenames).union(set(GM_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ddd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner','parser','lemmatizer','tagger', 'textcat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ee29b",
   "metadata": {},
   "source": [
    " ### Globe and Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49aaf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GM files proccesed: 0\n",
      "Number of GM files skipped: 151004\n",
      "Number of TS files proccesed: 41429\n",
      "Number of TS files skipped: 109575\n",
      "CPU times: user 35min 27s, sys: 13.1 s, total: 35min 40s\n",
      "Wall time: 39min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# We define a function to get the text content that we need from the XML articles available in our dataset\n",
    "def getxmlcontent(root):\n",
    "    if root.find('.//HiddenText') is not None:\n",
    "        return(root.find('.//HiddenText').text)\n",
    "    \n",
    "    elif root.find('.//Text') is not None:\n",
    "        return(root.find('.//Text').text)\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# The Globe and Mail\n",
    "count=0\n",
    "for filename in GM_filenames:\n",
    "    tree = etree.parse(GM_datapath + filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    if getxmlcontent(root) is not None:\n",
    "        soup = BeautifulSoup(getxmlcontent(root))\n",
    "        text = soup.get_text()\n",
    "    else:\n",
    "        text = 'Error in processing document'                    \n",
    "\n",
    "    vector_filename = filename[:-4]+ \".vector\"    \n",
    "    if not os.path.isfile(GloVe_datapath+vector_filename):\n",
    "        pickle.dump(nlp(text).vector,open(GloVe_datapath+vector_filename, \"wb\"))\n",
    "        count+=1\n",
    "print(f'Number of GM files proccesed: {count}')\n",
    "print(f'Number of GM files skipped: {len(GM_filenames)-count}')\n",
    "\n",
    "# The Globe and Mail\n",
    "count=0\n",
    "for filename in TS_filenames:\n",
    "    tree = etree.parse(TS_datapath + filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    vector_filename = filename[:-4]+ \".vector\"    \n",
    "    if not os.path.isfile(GloVe_datapath+vector_filename):\n",
    "        # Retrieve text\n",
    "        if getxmlcontent(root) is not None:\n",
    "            soup = BeautifulSoup(getxmlcontent(root))\n",
    "            text = soup.get_text()\n",
    "        else:\n",
    "            text = 'Error in processing document'  \n",
    "        # Process text and store vector\n",
    "        pickle.dump(nlp(text).vector,open(GloVe_datapath+vector_filename, \"wb\"))     \n",
    "        count+=1             \n",
    "\n",
    "\n",
    "        \n",
    "print(f'Number of TS files proccesed: {count}')\n",
    "print(f'Number of TS files skipped: {len(TS_filenames)-count}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58154f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vectores recuperados: 192,433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "vectors_datapath = '/home/ec2-user/SageMaker/mariano/notebooks/03. High Recall Retrieval System/vectors/'\n",
    "vector_filenames = [filename for filename in os.listdir(vectors_datapath) if filename.endswith('vector')]\n",
    "print(f'Cantidad de vectores recuperados: {len(vector_filenames):,}')\n",
    "\n",
    "id2vector = {}\n",
    "\n",
    "for vector_filename in vector_filenames:\n",
    "    id_ = int(vector_filename[:-7])\n",
    "    id2vector[id_] = pickle.load(open(vectors_datapath+vector_filename, 'rb'))\n",
    "    \n",
    "dict_path = vectors_datapath+'id2vec.p'\n",
    "pickle.dump(id2vector, open(dict_path, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848b41b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192433"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickle.load(open(dict_path,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47774250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
