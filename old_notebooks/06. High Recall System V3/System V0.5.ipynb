{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d22dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-25 20:36:29.266402 [  \u001b[1;92mOK\u001b[0m   ] Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from utils.io import info, ok, warning, html\n",
    "import os\n",
    "from utils.data_item import DataItem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import myversions.pigeonXT as pixt\n",
    "import pickle\n",
    "import warnings\n",
    "from math import ceil\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from utils.classifier import Classifier\n",
    "from utils.term_highlighter import TermHighlighter\n",
    "from joblib import load,dump\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import logging\n",
    "\n",
    "class HRSystem(object):\n",
    "    EXPANSION=10\n",
    "#     LABELING_THRESHOLD=0.90\n",
    "    RELEVANT_LABEL='Relevant'\n",
    "    IRRELEVANT_LABEL='Irrelevant'\n",
    "#     CACHE_PATH='./cache/'\n",
    "#     LABELED_PATH=CACHE_PATH+'labeled_data_not_incanada.p'       ## <<< MODIFIED\n",
    "#     UNLABELED_PATH=CACHE_PATH+'unlabeled_data.p'   ## <<< MODIFIED\n",
    "#     VECTOR_TYPE=DataItem.TYPE_BOW\n",
    "    VOCAB = np.array(open('../04. Model of DP/precomputed/vocab_with_dp.txt').read().splitlines())\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "#     MODELS_FILE = 'cache/models.joblib'\n",
    "#     HIGHLIGHTHER_FILE = 'cache/highlighter.joblib'\n",
    "    \n",
    "    \n",
    "    def _load_models(self):\n",
    "        if not os.path.isfile(self.models_path):\n",
    "            logging.debug('Creating the model from scratch. Need training.')\n",
    "            assert not os.path.isfile(self.highlighter_path)\n",
    "            self.classifiers = [Classifier(MLPClassifier( early_stopping=True,n_iter_no_change=20,max_iter=1500,hidden_layer_sizes=(20,), solver='adam'), type_=DataItem.TYPE_BOW),\n",
    "                                Classifier(MLPClassifier( early_stopping=True,n_iter_no_change=20,max_iter=1500,hidden_layer_sizes=(100,), solver='adam'), type_=DataItem.TYPE_GLOVE300),\n",
    "                          ]\n",
    "\n",
    "            self.term_highlighter = TermHighlighter()\n",
    "\n",
    "            logging.debug('Starting training of models.')\n",
    "            self._retrain(partial=False)\n",
    "            logging.debug('Finished training of models.')\n",
    "            \n",
    "        else:\n",
    "            logging.debug('Loading models from disk. No need for training.')\n",
    "            assert os.path.isfile(self.highlighter_path)\n",
    "            self._models_fromdisk()\n",
    "            \n",
    "    def _load_data(self,from_scratch):\n",
    "        ################\n",
    "        # LABELED DATA #\n",
    "        ################\n",
    "        if not from_scratch:\n",
    "            if not os.path.isfile(self.labeled_path):\n",
    "                logging.debug('Computing labeled data from \"labeled_data.csv\"')\n",
    "\n",
    "                self.labeled_data=[]\n",
    "                #############################\n",
    "                # LABELED OVER THREE ROUNDS #\n",
    "                #############################\n",
    "                for line in open('labeled_data.csv').read().splitlines()[1:]:\n",
    "                    id_,label = line.split(';')\n",
    "                    item = DataItem(id_)\n",
    "                    if label=='R':\n",
    "                        item.set_relevant()\n",
    "                    else:\n",
    "                        item.set_irrelevant()\n",
    "                        assert label=='I'\n",
    "                    if item.has_vector():\n",
    "                        self.labeled_data.append(item)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #############################################\n",
    "                # RETRIEVING FROM DISK INSTEAD OF COMPUTING #\n",
    "                #############################################\n",
    "                logging.debug(f'Retrieving labeled data from disk ({self.labeled_path})')\n",
    "                self.labeled_data = pickle.load(open(self.labeled_path,'rb'))\n",
    "            \n",
    "\n",
    "        ##################\n",
    "        # UNLABELED DATA #\n",
    "        ##################\n",
    "        GM1 = '/home/ec2-user/SageMaker/data/GM_all_1945_1956/'\n",
    "        GM2 = '/home/ec2-user/SageMaker/data/GM_all_1957-1967/'\n",
    "        if not os.path.isfile(self.unlabeled_path):\n",
    "            logging.debug(f'Computing unlabeled data from {GM1} and {GM2}')\n",
    "            self.unlabeled_data = [DataItem(GM1+file_) for file_ in os.listdir(GM1)] + [DataItem(GM2+file_) for file_ in os.listdir(GM2)]\n",
    "            relevant_ids = set([item.id_ for item in self.labeled_data])\n",
    "            self.unlabeled_data = [item for item in self.unlabeled_data if item.has_vector() and not item.id_ in relevant_ids]\n",
    "            \n",
    "        else:\n",
    "            #############################################\n",
    "            # RETRIEVING FROM DISK INSTEAD OF COMPUTING #\n",
    "            #############################################\n",
    "            logging.debug(f'Retrieving unlabeled data from disk ({self.unlabeled_path})')\n",
    "            self.unlabeled_data = pickle.load(open(self.unlabeled_path,'rb'))\n",
    "            \n",
    "        \n",
    "        self.rangen.shuffle(self.unlabeled_data)\n",
    "        \n",
    "        if from_scratch:\n",
    "            logging.debug('Creating labeled data from scratch')\n",
    "            self.labeled_data=[]\n",
    "            valid=False\n",
    "            while not valid:\n",
    "                seed = input('Please insert URLs to relevant document (; sep) (e.g., https://www.proquest.com/docview/1288605023/...)')\n",
    "                \n",
    "                matches = re.findall('docview/([0-9]*)/',seed)\n",
    "                logging.debug(f'User inputs: {seed}')\n",
    "                if len(matches)>=1:\n",
    "                    ids = set(matches)\n",
    "#                     print(ids)\n",
    "                    positions = [idx for idx,item in enumerate(self.unlabeled_data) if item.id_ in ids]\n",
    "                    if len(positions)>=1:\n",
    "                        for position in reversed(positions):\n",
    "                            self.labeled_data.append(self.unlabeled_data[position])\n",
    "                            self.labeled_data[-1].set_relevant()\n",
    "                            del(self.unlabeled_data[position])\n",
    "                        valid=True\n",
    "                        loging.debug(f\"Valid input, documents found: {','.join([item.id_ for item in self.labeled_data])}\")\n",
    "                    else:\n",
    "                        logging.debug('User query had something that look like valid documents but not present in database.')\n",
    "                        warning('Documents not found in database (The Globe and Mail 1936 onwards), please try again.')\n",
    "                else:\n",
    "                    logging.debug('User input does not look like valid URL.')\n",
    "                    warning('Invalid URLs, please try again.')\n",
    "    \n",
    "    ################################################################\n",
    "    #                         Constructor                          #\n",
    "    ################################################################\n",
    "    def __init__(self,from_scratch=False, session_name='default',labeling_batch=10):                                     \n",
    "        self.session_name = session_name\n",
    "        print('Starting system...'+' '*80, end='\\r')\n",
    "        if not os.path.exists(f'sessions/{session_name}'):\n",
    "            logging.debug('Session directories do not exist, creating (data, log, models).')\n",
    "            os.mkdir(f'sessions/{session_name}')\n",
    "            os.mkdir(f'sessions/{session_name}/data/')\n",
    "            os.mkdir(f'sessions/{session_name}/log/')\n",
    "            os.mkdir(f'sessions/{session_name}/models/')\n",
    "        self.labeled_path = f'sessions/{session_name}/data/labeled_data.p'\n",
    "        self.unlabeled_path = f'sessions/{session_name}/data/unlabeled_data.p'\n",
    "        self.models_path = f'sessions/{session_name}/models/models.joblib'\n",
    "        self.highlighter_path = f'sessions/{session_name}/models/highlighter.joblib'\n",
    "        self.iteration_counter_path = f'sessions/{session_name}/data/iteration_counter.p'\n",
    "\n",
    "        logging.basicConfig(filename=f'sessions/{session_name}/log/system.log', \n",
    "                            format='%(asctime)s [%(levelname)s] %(message)s' ,\n",
    "                            encoding='utf-8', \n",
    "                            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                            force=True,\n",
    "                            level=logging.DEBUG)\n",
    "        if os.path.isfile(self.iteration_counter_path):\n",
    "            self.iteration_count = pickle.load(open(self.iteration_counter_path,'rb'))\n",
    "            logging.debug(f'Iteration counter retrieved from disk, iteration count={self.iteration_count}')\n",
    "        else:\n",
    "            self.iteration_count = 0\n",
    "                                     \n",
    "        logging.debug(f'System starting. Session name={session_name}. Iteration count={self.iteration_count}')\n",
    "        self.remaining_relevant=None\n",
    "        self.rangen = np.random.default_rng(2022)\n",
    "        self.suggestions=[]\n",
    "        self.annotations = pd.DataFrame([],columns=['label'])\n",
    "        self.labeling_batch=labeling_batch\n",
    "                                     \n",
    "        print('Loading data...'+' '*80, end='\\r')\n",
    "        self._load_data(from_scratch)\n",
    "        logging.debug(f\"len(labeled_data)={len(self.labeled_data)} {self._labeled_data_str()}\")\n",
    "        logging.debug(f\"len(unlabeled_data)={len(self.unlabeled_data)} {self._unlabeled_data_str()}\")\n",
    "#                                                                               for item in self.labeled_path[:2]+\\\n",
    "#                                                                              self.labeled_path[]+])}\")\n",
    "#         logging.debug(f'len(unlabeled_data)={len(self.unlabeled_data)} ()')\n",
    "        ################################\n",
    "        # PRELOAD LABELED DATA VECTORS #\n",
    "        ################################\n",
    "        for item in self.labeled_data:\n",
    "            item.preload_vector(type_=DataItem.TYPE_GLOVE300) \n",
    "            item.preload_vector(type_=DataItem.TYPE_BOW)       \n",
    "        \n",
    "\n",
    "        print('Loading/training models...'+' '*80, end='\\r')\n",
    "        self._load_models()\n",
    "        print('System started. Saving session and computing status...')\n",
    "        self.save()\n",
    "        self.status()\n",
    "                                     \n",
    "        logging.debug('System started succesfully.')\n",
    "        \n",
    "    ################################################################\n",
    "    #                   save/load models                           #\n",
    "    ################################################################  \n",
    "    def _models_todisk(self):\n",
    "        for file_ in [self.models_path,self.highlighter_path ]:\n",
    "            if os.path.isfile(file_):\n",
    "                logging.debug(f'Overwriting file {file_}')\n",
    "            else:\n",
    "                logging.debug(f'Creating file {file_}')\n",
    "        dump(self.classifiers, self.models_path)\n",
    "        dump(self.term_highlighter, self.highlighter_path)\n",
    "                                     \n",
    "    def _models_fromdisk(self):\n",
    "        logging.debug(f'Retrieving {self.highlighter_path} and {self.models_path} from disk')\n",
    "        self.term_highlighter = load(self.highlighter_path)\n",
    "        self.classifiers = load(self.models_path)\n",
    "        \n",
    "    ################################################################\n",
    "    #                          Re-Train                            #\n",
    "    ################################################################  \n",
    "    def _retrain(self, partial=True): \n",
    "        expanded=False\n",
    "        if len(self.labeled_data)<10:\n",
    "\n",
    "            expanded=True            \n",
    "            self.labeled_data += [DataItem(item.id_) for item in self.rangen.choice(\n",
    "                                                                                   self.unlabeled_data,\n",
    "                                                                                   size=HRSystem.EXPANSION,\n",
    "                                                                                   replace=False)]\n",
    "                                     \n",
    "            logging.debug(f'Only {len(self.labeled_data)} articles labeled. Expanding labeled data'\\\n",
    "                         f' (adding {HRSystem.EXPANSION} randomly selected negative examples)'\\\n",
    "                         f\" ({','.join([str(item.id_) for item in self.labeled_data[-HRSystem.EXPANSION:]])})\")\n",
    "                                     \n",
    "            for item in self.labeled_data[-HRSystem.EXPANSION:]:\n",
    "                item.set_irrelevant()\n",
    "        logging.debug(f'Training each of the {len(self.classifiers)} classifiers.')\n",
    "        for clf in self.classifiers:\n",
    "#             info(f'Training: {str(clf)}')\n",
    "            logging.debug(f'Training model {str(clf)}')\n",
    "            if not partial:\n",
    "                clf.fit(self.labeled_data)\n",
    "            else:\n",
    "                clf.fit(self.labeled_data)\n",
    "            \n",
    "        logging.debug('Training term highlighter.')\n",
    "        self.term_highlighter.fit(self.labeled_data)\n",
    "        logging.debug('Done training.')\n",
    "                     \n",
    "        \n",
    "        if expanded:\n",
    "                logging.debug('Removing randomly selected negatives examples (expansion)'\\\n",
    "                             f\" ({','.join([str(item.id_) for item in self.labeled_data[-HRSystem.EXPANSION:]])})\")\n",
    "                                     \n",
    "                \n",
    "                self.labeled_data = self.labeled_data[:-HRSystem.EXPANSION]\n",
    "                logging.debug(f'Size of labeled data after removing expansion={len(self.labeled_data)}')\n",
    "    \n",
    "    def _move_suggestions_to_labeled(self): \n",
    "        assert len(self.suggestions)>0\n",
    "\n",
    "        logging.debug(f'There are {len(self.suggestions)} suggestions labeled that need to be moved to labeled_data. Re-train required.')\n",
    "        logging.info(f'len(suggestions)={len(self.suggestions)} {self._suggestions_str()}')\n",
    "        logging.info(f'Annotations: '+','.join(self.annotations[\"label\"]))\n",
    "        logging.debug('Moving from suggestions --to--> labeled data (for latter training)')\n",
    "        need_retrain=True\n",
    "        \n",
    "        relevant_count=0\n",
    "            \n",
    "        for item,label in zip(self.suggestions, self.annotations[\"label\"]):\n",
    "            if label==HRSystem.RELEVANT_LABEL:\n",
    "                item.set_relevant()\n",
    "                relevant_count+=1\n",
    "            else:\n",
    "                item.set_irrelevant()\n",
    "                assert label==HRSystem.IRRELEVANT_LABEL\n",
    "        logging.info(f'From the {len(self.suggestions)} suggestions {relevant_count} where found relevant.'\\\n",
    "                     f' ({relevant_count/len(self.suggestions):5.4f})')\n",
    "        \n",
    "        self.labeled_data = self.labeled_data+self.suggestions\n",
    "        logging.debug(f\"new len(labeled_data)={len(self.labeled_data)} {self._labeled_data_str()}\")\n",
    "        logging.debug(f\"new len(unlabeled_data)={len(self.unlabeled_data)} {self._unlabeled_data_str()}\")\n",
    "                                     \n",
    "        del(self.annotations)\n",
    "    \n",
    "        print('Re-training models using new suggestions ',end='\\r')\n",
    "        logging.debug(f'Re-training model ussing new {len(self.suggestions)} suggestions.')\n",
    "        self._retrain()\n",
    "    ################################################################ \n",
    "    #                        SAVE LISTS                            #   \n",
    "    ################################################################                                                                                                   'relevant (estimated)')        \n",
    "    def save(self):   \n",
    "        if len(self.suggestions)>0:\n",
    "            logging.debug('Attemping to save system\\'s state but there are labeled suggestions to be stored first. Re-train required.')\n",
    "            self._move_suggestions_to_labeled()\n",
    "        else:\n",
    "            logging.debug('Saving system\\'s state. NO suggestions pending to be moved to labeled data.')\n",
    "                                     \n",
    "        for file_ in [self.labeled_path,self.unlabeled_path,self.iteration_counter_path]:\n",
    "            if os.path.isfile(file_):\n",
    "                logging.debug(f'Overwriting file {file_}')\n",
    "            else:\n",
    "                logging.debug(f'Creating file {file_}')\n",
    "            \n",
    "\n",
    "                                     \n",
    "                                     \n",
    "        pickle.dump(self.labeled_data, open(self.labeled_path, 'wb'))\n",
    "        pickle.dump(self.unlabeled_data, open(self.unlabeled_path, 'wb'))\n",
    "        pickle.dump(self.iteration_count, open(self.iteration_counter_path,'wb'))\n",
    "        self._models_todisk()\n",
    "    \n",
    "    def _relevant_count(self):\n",
    "        return len([item for item in self.labeled_data if item.label==DataItem.REL_LABEL])\n",
    "        \n",
    "    def _labeled_count(self):\n",
    "        return len(self.labeled_data)\n",
    "    def _unlabeled_count(self):\n",
    "        return len(self.unlabeled_data)\n",
    "    \n",
    "    \n",
    "    def _filter_and_sort_candidates(self,candidate_args):\n",
    "\n",
    "                                     \n",
    "        # First Model\n",
    "        \n",
    "        yhat1 = self.classifiers[0].predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "        candidate_args = np.array(candidate_args)[yhat1>0.5]\n",
    "        logging.debug(f'Number of suggestions 1st model: {np.sum(yhat1>0.5)} (discarded: {yhat1<=0.5})')\n",
    "        \n",
    "                         \n",
    "        # Second Model\n",
    "        yhat1 = yhat1[yhat1>0.5]\n",
    "        candidate_args = np.array(candidate_args)[yhat1>0.5]\n",
    "        yhat1 = yhat1[yhat1>0.5]\n",
    "        yhat2 = self.classifiers[1].predict([self.unlabeled_data[arg] for arg in candidate_args])   \n",
    "        logging.debug(f'Number of suggestions 2nd model: {np.sum(yhat2>0.5)} (discarded: {yhat2<=0.5})')\n",
    "                                     \n",
    "        # Third Model\n",
    "        mask = (yhat2>0.5) #& (yhat3>0.5)\n",
    "        yhat1 = yhat1[mask]\n",
    "        yhat2 = yhat2[mask]\n",
    "        candidate_args = np.array(candidate_args)[mask]\n",
    "        yhat4 = self.term_highlighter.predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "        logging.debug(f'Number of suggestions 3rd model: {np.sum(yhat4>0.5)} (discarded: {yhat4<=0.5})')\n",
    "                                     \n",
    "        # Average\n",
    "        yhat = np.average(np.vstack([yhat1,yhat2,yhat4]), axis=0)                                     \n",
    "        yhat = yhat[yhat4>0.5]\n",
    "        candidate_args = np.array(candidate_args)[yhat4>0.5]\n",
    "        candidate_args = np.array(candidate_args)[np.argsort(yhat)[::-1]]\n",
    "               \n",
    "        # Info for debugging\n",
    "        end = min(len(candidate_args),self.labeling_batch)\n",
    "        confidence_levels = [f'{yhat[arg]:4.3f}' for arg in np.argsort(yhat)[::-1]]\n",
    "        logging.debug(f\"Confidence levels for suggestions: {','.join(confidence_levels[:10])} \")\n",
    "                      \n",
    "        return candidate_args, yhat\n",
    "                                     \n",
    "    def _compute_suggestions(self):\n",
    "#         cap = min(50000,len(self.unlabeled_data))\n",
    "#         logging.debug(f'Computing suggestions. Calculating predictions for {cap} unlabeled articles.')\n",
    "# #         info('Re-trained. Computing suggestions...')\n",
    "        \n",
    "#         candidate_args = self.rangen.choice(range(len(self.unlabeled_data)), size=cap, replace=False )\n",
    "# #         info(f'Making predictions over {len(candidate_args)} with model: {self.classifiers[0]}')\n",
    "#         logging.debug(f\"Predicting with \" +str(self.classifiers[0]).replace('\\n','') )\n",
    "#         print('Predicting with first model',end='\\r')\n",
    "#         yhat1 = self.classifiers[0].predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "# #         print(f'yhat1.shape={yhat1.shape}')\n",
    "#         logging.debug(f'Only {np.sum(yhat1>0.5)} articles suggested found with first classifier ({len(candidate_args)-np.sum(yhat1>0.5)} discarded)')\n",
    "        \n",
    "# #         info(f'Descarding {len(candidate_args)-np.sum(yhat1>0.5)} articles.')\n",
    "#         print(f'Predicting with second model ({len(candidate_args)-np.sum(yhat1>0.5)} discarded)',end='\\r')\n",
    "#         candidate_args = np.array(candidate_args)[yhat1>0.5]\n",
    "#         yhat1 = yhat1[yhat1>0.5]\n",
    "        \n",
    "# #         info(f'Making predictions over {len(candidate_args)} with model: {self.classifiers[1]}')\n",
    "#         logging.debug(f\"Predicting with \" +str(self.classifiers[1]).replace('\\n','') )\n",
    "#         yhat2 = self.classifiers[1].predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "#         logging.debug(f'Only {np.sum(yhat2>0.5)} articles suggested found with second classifier ({len(candidate_args)-np.sum(yhat2>0.5)} discarded)')\n",
    "                   \n",
    "# #         yhat3 = self.classifiers[2].predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "        \n",
    "        \n",
    "# #         info(f'Descarding {len(candidate_args)-np.sum(yhat2>0.5)} articles.')\n",
    "#         mask = (yhat2>0.5) #& (yhat3>0.5)\n",
    "        \n",
    "#         yhat1 = yhat1[mask]\n",
    "#         yhat2 = yhat2[mask]\n",
    "# #         yhat3 = yhat3[mask]\n",
    "# #         print(f'yhat1.shape={yhat1.shape}')\n",
    "# #         print(f'yhat2.shape={yhat2.shape}')\n",
    "# #         print(f'yhat3.shape={yhat3.shape}')\n",
    "#         print(f'Predicting with last model ({len(candidate_args)-np.sum(yhat1>0.5)} discarded)',end='\\r')\n",
    "#         candidate_args = np.array(candidate_args)[mask]\n",
    "        \n",
    "#         logging.debug(f'Predicting with term highlighter ({str(self.term_highlighter.model)})')\n",
    "\n",
    "# #         info(f'Making predictions over {len(candidate_args)} with model: {self.term_highlighter}')\n",
    "#         yhat4 = self.term_highlighter.predict([self.unlabeled_data[arg] for arg in candidate_args])\n",
    "#         logging.debug(f'Only {np.sum(yhat4>0.5)} articles suggested found with second classifier ({len(candidate_args)-np.sum(yhat4>0.5)} discarded)')\n",
    "\n",
    "# #         print(f'yhat4.shape={yhat4.shape}')\n",
    "#         yhat = np.average(np.vstack([yhat1,yhat2,yhat4]), axis=0)\n",
    "# #         yhat = np.average(np.vstack([yhat1,yhat2,yhat4]), axis=0)\n",
    "        \n",
    "#         candidate_args = np.array(candidate_args)[np.argsort(yhat)[::-1]]\n",
    "        cap = min(50000,len(self.unlabeled_data))\n",
    "                                     \n",
    "        self.estimated=False\n",
    "        if cap!=len(self.unlabeled_data):\n",
    "                  self.estimated=True\n",
    "                                     \n",
    "        logging.debug(f'Computing batch of suggestions, batch_size={cap}')\n",
    "                      \n",
    "        candidate_args = self.rangen.choice(range(len(self.unlabeled_data)), size=cap, replace=False )  \n",
    "        candidate_args, _ = self._filter_and_sort_candidates(candidate_args)\n",
    "                                     \n",
    "\n",
    "                  \n",
    "        self.remaining_relevant = len(candidate_args) #np.sum(yhat4>0.5) #np.sum(yhat>0.5)\n",
    "        logging.debug(f'Relevant found in the {cap} articles analyzed: {self.remaining_relevant} ')\n",
    "        if self.estimated:\n",
    "            self.remaining_relevant = int((self.remaining_relevant/cap)*len(self.unlabeled_data))\n",
    "            logging.debug(f'Estimated relevant in the remaining {len(self.unlabeled_data)} articles: {self.remaining_relevant} ')\n",
    "#             self.remaining_relevant = estimated_relevants\n",
    "\n",
    "        # Remove most promising from unlabeled \n",
    "        #   and add to suggestions\n",
    "        \n",
    "        ################\n",
    "        ## UNTIL HERE ##\n",
    "        ################\n",
    "        \n",
    "        self.suggestions = []\n",
    "        self.annotations = []\n",
    "        \n",
    "        if self.remaining_relevant==0:\n",
    "            logging.debug('No good candidates (predictions<0.5).')\n",
    "            warning('There are no good candidates provided by the model. '\\\n",
    "            'This could happend at the beginin and at the end of the labeling process')\n",
    "            \n",
    "        end = min(len(candidate_args),self.labeling_batch)\n",
    "        best_ten_args = candidate_args[:end]\n",
    "\n",
    "#         best_ten_args = np.argsort(yhat)[-start:][::-1]\n",
    "\n",
    "        for arg in best_ten_args:\n",
    "            self.suggestions.append(self.unlabeled_data[arg])\n",
    "    \n",
    "        for arg in sorted(best_ten_args,reverse=True):\n",
    "            del(self.unlabeled_data[arg])\n",
    "        \n",
    "        logging.debug('Moving the suggestions made by the model from unlabeled_data --to--> suggestions')              \n",
    "        logging.debug(f\"new len(unlabeled_data)={len(self.unlabeled_data)} {self._unlabeled_data_str()}\")\n",
    "        logging.debug(f'new len(suggestions)={len(self.suggestions)} {self._suggestions_str()}')\n",
    "\n",
    "        for classifier in self.classifiers:\n",
    "            for item in self.suggestions:\n",
    "                item.preload_vector(type_=classifier.vector_type)\n",
    "                item.preload_vector(type_=classifier.vector_type)\n",
    "        \n",
    "\n",
    "#         info(f'Moving {len(self.suggestions)} unlabeled suggestions'\\\n",
    "#              f' from unlabeled data ({len(self.unlabeled_data)} - {len(self.suggestions)})')\n",
    "    ################################################################ \n",
    "    #                             LOOP                             #   \n",
    "    ################################################################  \n",
    "    def loop(self, finish_function=None):        \n",
    "        ####################################################\n",
    "        # MOVING FROM SUGGESTIONS (ANNOTATIONS) TO LABELED #\n",
    "        ####################################################\n",
    "        logging.debug(f'Starting loop no {self.iteration_count}')\n",
    "        need_retrain=False\n",
    "        if len(self.suggestions)>0:\n",
    "            self._move_suggestions_to_labeled()\n",
    "        \n",
    "        print('Calculating new suggestions',end='\\r')\n",
    "        self._compute_suggestions()\n",
    "        \n",
    "        highlighter = None\n",
    "        if self.term_highlighter.trained:\n",
    "            logging.debug(f\"Highlighting with {','.join(self.term_highlighter.sorted_terms()[:10])},...\")\n",
    "            highlighter = self.term_highlighter\n",
    "        text_for_label = [suggestion.get_htmldocview(highlighter=highlighter)\n",
    "                          for suggestion in self.suggestions]\n",
    "\n",
    "        print()\n",
    "        self.status()\n",
    "        self.annotations = pixt.annotate(\n",
    "                                         text_for_label,\n",
    "                                         options=[HRSystem.RELEVANT_LABEL, HRSystem.IRRELEVANT_LABEL],\n",
    "                                         stop_at_last_example=False,\n",
    "                                         display_fn=html,\n",
    "                                         final_process_fn=finish_function\n",
    "                                        )\n",
    "        self.iteration_count+=1\n",
    "                          \n",
    "    def review_labeled(self, how_many=20):\n",
    "        highlighter = None\n",
    "        if self.term_highlighter.trained:\n",
    "            highlighter = self.term_highlighter\n",
    "        text_for_label = [suggestion.get_htmldocview(highlighter=highlighter)\n",
    "                          for suggestion in self.labeled_data[-how_many:]]\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "                       {\n",
    "                        'example': text_for_label,\n",
    "                        'changed':[True]*how_many,\n",
    "                        'label':[HRSystem.RELEVANT_LABEL if item.label==DataItem.REL_LABEL else HRSystem.IRRELEVANT_LABEL  \n",
    "                                 for item in self.labeled_data[-how_many:] ]\n",
    "                       }\n",
    "                      )\n",
    "        self.annotations = pixt.annotate(\n",
    "                                         df,\n",
    "                                         options=[HRSystem.RELEVANT_LABEL, HRSystem.IRRELEVANT_LABEL],\n",
    "                                         stop_at_last_example=False,\n",
    "                                         display_fn=html,\n",
    "#                                          final_process_fn=finish_function\n",
    "                                        )\n",
    "        \n",
    "        \n",
    "        # CORREGIR self.labeled.\n",
    "        # REPORTAR CNATIDAD DE CAMBIOS. \n",
    "        \n",
    "        # if changes need retrain. Do here?\n",
    "        \n",
    "        \n",
    "    ################################################################ \n",
    "    #                            EXPORT                            #   \n",
    "    ################################################################  \n",
    "    def export(self):\n",
    "        if len(self.suggestions)>0:\n",
    "            logging.debug('Attemping to export system\\'s state but there are labeled suggestions to be stored first. Re-train required.')\n",
    "            self._move_suggestions_to_labeled()\n",
    "        else:\n",
    "            logging.debug('Exporting... (NO suggestions pending to be moved to labeled data)')\n",
    "                      \n",
    "        filename = f'sessions/{self.session_name}/data/exported_data_'+time.strftime(\"%Y-%m-%d_%H-%M\")+'.csv'\n",
    "        with open(filename, 'w') as writer:\n",
    "            writer.write('URL,relevant_or_suggested,confidence\\n')\n",
    "            for item in self.labeled_data:\n",
    "                if item.is_relevant():\n",
    "                    writer.write(f'https://proquest.com/docview/{item.id_},relevant,1\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "            # MAKE PREDICTIONS AND STORE SUGGESTIONS....\n",
    "            batch= 20000\n",
    "            suggestions = []\n",
    "            for i in range(0, len(self.unlabeled_data),batch):\n",
    "                ini = i\n",
    "                fin = min(ini+batch,len(self.unlabeled_data))\n",
    "                args = list(range(ini,fin))\n",
    "                candidate_args, yhat = self._filter_and_sort_candidates(candidate_args)\n",
    "                suggestions = list(zip([self.unlabeled_data[arg] for arg in candidate_args], yhat))\n",
    "            suggestions = sorted(suggestions, key=lambda x: x[1], reverse=True)\n",
    "            for item,confidence in suggestions:\n",
    "                writer.write(f'https://proquest.com/docview/{item.id_},suggested,{confidence:4.3f}\\n')\n",
    "        # System call to send file\n",
    "        \n",
    "    ################################################################ \n",
    "    #                            STATUS                            #   \n",
    "    ################################################################ \n",
    "    def _center(str_,width=100):\n",
    "        width_aux =width-2\n",
    "        out =  '#'+' '*(int((width_aux-len(str_))/2))+str_+' '*(int((width_aux-len(str_))/2))+'#'\n",
    "        if len(out)!=width:\n",
    "            out = out[:-1]+' #'\n",
    "        return out\n",
    "    def _left(str_,width=100):\n",
    "        output =  '# '+str_\n",
    "        remaining = width-len(output)\n",
    "        return output+' '*(remaining-1)+'#'\n",
    "    def status(self):\n",
    "        width = 100\n",
    "        print('#'*width)\n",
    "\n",
    "        print(HRSystem._center('~~~~~~~~~~~~~~~~~~'))\n",
    "        print(HRSystem._center('~ System Status: ~'))\n",
    "        print(HRSystem._center('~~~~~~~~~~~~~~~~~~'))\n",
    "        print(HRSystem._left(f'Number of labeled articles:   {len(self.labeled_data):10,}    -    {self._relevant_count():10,} relevant '\\\n",
    "        f'   {len(self.labeled_data)-self._relevant_count():10,} irrelevants'))\n",
    "        if self.remaining_relevant is None:\n",
    "            print(HRSystem._left(f'Number of unlabeled articles: {len(self.unlabeled_data):10,}    -'+' '*10+' N/A suggestions '))\n",
    "        else:\n",
    "            print(HRSystem._left(f'Number of unlabeled articles:   {len(self.unlabeled_data):10,}  -    {self.remaining_relevant:10,} suggestions '\\\n",
    "            f'{len(self.unlabeled_data)-self.remaining_relevant:10,} irrelevants'))\n",
    "            if self.estimated:\n",
    "                print(HRSystem._left(' '*60+'(ESTIMATED)'))\n",
    "        print(HRSystem._center(''))\n",
    "        \n",
    "        ytrue = DataItem.get_y(self.labeled_data)\n",
    "        for model in self.classifiers + [self.term_highlighter]:\n",
    "            print(HRSystem._left(str(model.model).replace('\\n','').replace('  ','')))\n",
    "            print(HRSystem._left('~'*len(str(model.model).replace('\\n','').replace('  ',''))))\n",
    "\n",
    "            yhat = model.predict(self.labeled_data)>0.5\n",
    "            yhat = yhat.astype('int')\n",
    "            scores = model.cross_validate_on(self.labeled_data,cv=3)\n",
    "            metrics = [\n",
    "                       scores['train_accuracy'],\n",
    "                       scores['train_precision'],\n",
    "                       scores['train_recall'],\n",
    "                       scores['train_f1'],\n",
    "                       ]\n",
    "            performance = f'{str(model)};train_accuracy:{metrics[0]};train_precision:{metrics[1]};'\n",
    "            performance += f'train_recall:{metrics[2]};train_f1:{metrics[3]};'\n",
    "            logging.info(performance)\n",
    "            print(HRSystem._left('accuracy   precision      recall      f1-score'))\n",
    "            print(HRSystem._left('  '+'      '.join(f'{metric:5.4f}' for metric in metrics) +' (train)'))\n",
    "            metrics = [\n",
    "                       scores['test_accuracy'],\n",
    "                       scores['test_precision'],\n",
    "                       scores['test_recall'],\n",
    "                       scores['test_f1'],\n",
    "                       ]\n",
    "            performance = f'{str(model)};test_accuracy:{metrics[0]};test_precision:{metrics[1]};'\n",
    "            performance +=f'test_recall:{metrics[2]};test_f1:{metrics[3]};'\n",
    "            logging.info(performance)\n",
    "            print(HRSystem._left('  '+'      '.join(f'{metric:5.4f}' for metric in metrics)+' (test )'))\n",
    "            tn, fp, fn, tp = (confusion_matrix(ytrue,yhat)).ravel()\n",
    "            print(HRSystem._center('~~~~~~~~~~~~~~~~~~~~~~~~~~'))\n",
    "            print(HRSystem._center('~~~~~Confusion Matrix~~~~~'))\n",
    "            print(HRSystem._center('~~~~~~~~~~~~~~~~~~~~~~~~~~'))\n",
    "            print(HRSystem._center(f'TN = {tn:6,}    FP = {fp:6,}'))\n",
    "            print(HRSystem._center(f'FN = {fn:6,}    TP = {tp:6,}'))\n",
    "            print(HRSystem._center(' '))\n",
    "        print('#'*width)\n",
    "    def _labeled_data_str(self):\n",
    "        str_ = '<'+','.join([item.id_ for item in self.labeled_data[:2]])\n",
    "        str_ +=', ... ,'\n",
    "        str_ +=','.join([item.id_ for item in self.labeled_data[-2:]])+'>'\n",
    "        return str_\n",
    "    def _unlabeled_data_str(self):\n",
    "        str_ = '<'+','.join([item.id_ for item in self.unlabeled_data[:2]])\n",
    "        str_ +=', ... ,'\n",
    "        str_ +=','.join([item.id_ for item in self.unlabeled_data[-2:]])+'>'\n",
    "        return str_\n",
    "    def _suggestions_str(self):\n",
    "        str_ = '<'+','.join([item.id_ for item in self.suggestions])+'>'\n",
    "        return str_\n",
    "\n",
    "ok('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4ca42c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e061013de3e4250a3d83fac581c4939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='RE-INIT', disabled=True, style=ButtonStyle()), Button(description='LOOP', dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating new suggestions\r"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import (\n",
    "        Button,\n",
    "        Dropdown,\n",
    "        HTML,\n",
    "        HBox,\n",
    "        VBox,\n",
    "        IntSlider,\n",
    "        FloatSlider,\n",
    "        Textarea,\n",
    "        Output,\n",
    "        ToggleButton\n",
    ")\n",
    "data = []\n",
    "session_name = 'test'\n",
    "from_scratch = False\n",
    "\n",
    "def disable_buttons():\n",
    "    for button in buttons:\n",
    "        button.disabled=True\n",
    "def enable_buttons():\n",
    "    for button in buttons:\n",
    "        button.disabled=False\n",
    "        \n",
    "def on_click_init(button=None):\n",
    "    clear_output(wait=False)\n",
    "    disable_buttons()\n",
    "    display(HBox(buttons))\n",
    "    system = HRSystem(from_scratch=from_scratch, session_name=session_name)\n",
    "    buttons[0].description='RE-INIT'\n",
    "    data.append(system)\n",
    "    if len(data)==2:\n",
    "        del(data[0])\n",
    "    enable_buttons()    ## THIS BUTTON SHOULD BE ENABLED FROM THE annotation.py library!!! (PIXT)\n",
    "def on_click_loop(button=None):\n",
    "    clear_output(wait=False)\n",
    "    disable_buttons()\n",
    "    display(HBox(buttons))\n",
    "    system = data[0]\n",
    "    system.loop(finish_function=enable_buttons)\n",
    "def on_click_save(button=None):\n",
    "    clear_output(wait=False)\n",
    "    disable_buttons()\n",
    "    display(HBox(buttons))\n",
    "    system = data[0]\n",
    "    system.save()\n",
    "    enable_buttons()\n",
    "def on_click_export(button=None):\n",
    "    clear_output(wait=False)\n",
    "    disable_buttons()\n",
    "    display(HBox(buttons))\n",
    "    system = data[0]\n",
    "    system.export()\n",
    "    enable_buttons()\n",
    "def on_click_review(button=None):\n",
    "    pass\n",
    "descriptions = ['INIT', 'LOOP', 'SAVE', 'EXPORT', 'REVIEW']\n",
    "on_click_functions = [on_click_init, on_click_loop, on_click_save, on_click_export,on_click_review]\n",
    "buttons = [Button() for i in range(len(descriptions))]\n",
    "\n",
    "for idx,button in enumerate(buttons):\n",
    "    button.description = descriptions[idx]\n",
    "    button.disabled=False\n",
    "    button.on_click(on_click_functions[idx])\n",
    "\n",
    "for i in range(4):\n",
    "    buttons[i+1].disabled=True\n",
    "    \n",
    "\n",
    "display(HBox(buttons))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5e1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
