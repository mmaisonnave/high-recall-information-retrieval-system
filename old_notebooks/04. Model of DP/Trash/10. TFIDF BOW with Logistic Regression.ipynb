{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import info, ok, warning, id2file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9fc2c",
   "metadata": {},
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f80296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "relevant_set = set()\n",
    "irrelevant_set = set()\n",
    "\n",
    "# Loading new_data\n",
    "new_data = [line.split(';') for line in open('new_data.csv').read().splitlines()]\n",
    "relevant_set = relevant_set.union(set([id_ for id_,label in new_data if label.strip()=='R']))\n",
    "irrelevant_set = irrelevant_set.union(set([id_ for id_,label in new_data if label.strip()=='I']))\n",
    "\n",
    "# Loading original data\n",
    "DP_examples_dirpath = '/home/ec2-user/SageMaker/mariano/notebooks/04. Model of DP/DP-relevant articles/'\n",
    "\n",
    "first_data = []\n",
    "for dirpath, dirnames, filenames in os.walk(DP_examples_dirpath):\n",
    "    for filename in filenames:\n",
    "        content = open(os.path.join(dirpath,filename),'r').read()\n",
    "        ids = re.findall('/docview/([^/]*)/',content)\n",
    "        relevant_set = relevant_set.union(set(ids))\n",
    "    \n",
    "# articles containg DP and Canada from that period, that were not deteted by Serperi\n",
    "GM_dp_dirpath = '/home/ec2-user/SageMaker/data/GM_DP_and_Canada1945_1967/'\n",
    "\n",
    "files = os.listdir(GM_dp_dirpath)\n",
    "\n",
    "irrelevant_set = irrelevant_set.union([file_[:-4] for file_ in files if file_[:-4] not in relevant_set and file_.endswith('.xml')])\n",
    "\n",
    "not_found=[]\n",
    "for id_ in list(relevant_set)+list(irrelevant_set):\n",
    "    if id2file(id_) is None:\n",
    "        not_found.append(id_)\n",
    "print(f'Not found: {not_found}')\n",
    "for id_ in not_found:\n",
    "    relevant_set = relevant_set.difference(set(not_found))\n",
    "    irrelevant_set = irrelevant_set.difference(set(not_found))\n",
    "    \n",
    "info(f'len(relevant_set)   = {len(relevant_set)}')\n",
    "info(f'len(irrelevant_set) = {len(irrelevant_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.models import tokenize\n",
    "from utils.tdmstudio import TDMStudio\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat', 'parser','ner'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                             input='content',\n",
    "                             lowercase=True,\n",
    "                             preprocessor=None,\n",
    "                             tokenizer=tokenize,\n",
    "                             analyzer='word',\n",
    "                             stop_words=list(nlp.Defaults.stop_words),\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b\", #selects tokens of 2 or more alphanumeric char (punctuation is completely ignored and treated as token separator)\n",
    "                             ngram_range=(1,1), #lower and upper boundary of the range of n-values for different n-grams.\n",
    "                             max_df=1.0, #ignore terms that have a document frequency strictly higher than given threshold\n",
    "                             min_df=0.001, #ignore terms that have a document frequency strictly lower than given threshold\n",
    "                             max_features=50000, #build a vocabulary that only considers the top max_features ordered by term frequency acoss the corpus\n",
    "                             vocabulary=None, #vocabulary is determined from the input documents\n",
    "                             norm='l2',\n",
    "                             use_idf=True,\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return ''.join([char for char in word if not char in string.punctuation+' '])\n",
    "\n",
    "def tokenize(str_):\n",
    "    tokens = [word.lemma_.lower() for word in nlp(str_) if not word.is_stop]\n",
    "    tokens = [word.replace('\\n', '') for word in tokens if not word.isnumeric() and len(remove_punctuation(word))!=0]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "corpus = [TDMStudio.get_title_and_text(id2file(id_)) for id_ in list(relevant_set)+list(irrelevant_set)]\n",
    "corpus = [f'{title}. {text}' for title,text in corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74effb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info('Starting fit...')\n",
    "vectorizer.fit(corpus)\n",
    "info('Getting vocab...')\n",
    "vocab = vectorizer.vocabulary_\n",
    "info('Creatin X...')\n",
    "X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.zeros(shape=(X.shape[0],))\n",
    "y[:len(relevant_set)]=1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3402d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(\n",
    "                         penalty='l2',\n",
    "                         dual=True, # Prefer dual=False when n_samples>n_features\n",
    "                         tol=1e-4,  # tolerance\n",
    "                         C=1,       # Regularization strength. Smaller value specify stronger regularization\n",
    "                         fit_intercept=True, \n",
    "                         intercept_scaling=1,\n",
    "                         class_weight=None,\n",
    "                         solver='lbfgs', #\n",
    "                         n_jobs=3,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
