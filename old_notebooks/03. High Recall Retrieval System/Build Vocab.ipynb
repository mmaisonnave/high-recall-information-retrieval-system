{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030dea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "TS_data_path = '/home/ec2-user/SageMaker/data/Toronto_Star_Publication_with_query/'\n",
    "GM_data_path = '/home/ec2-user/SageMaker/data/The_Globe_and_Mail_with_DP_filter_by_article_type/'\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(TS_data_path):\n",
    "    filenames.append(TS_data_path+filename)\n",
    "    \n",
    "for filename in os.listdir(GM_data_path):\n",
    "    filenames.append(GM_data_path+filename)\n",
    "    \n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287e56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat','ner','lemmatizer','parser','tagger'])\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd38fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed        0 of  192,433 (2022-02-02 18:03:28.490711)\n",
      "Processed   10,000 of  192,433 (2022-02-02 18:04:09.802195)\n",
      "Processed   20,000 of  192,433 (2022-02-02 18:04:44.722166)\n",
      "Processed   30,000 of  192,433 (2022-02-02 18:05:17.298169)\n",
      "Processed   40,000 of  192,433 (2022-02-02 18:05:48.787908)\n",
      "Processed   50,000 of  192,433 (2022-02-02 18:07:13.301679)\n",
      "Processed   60,000 of  192,433 (2022-02-02 18:08:38.424454)\n",
      "Processed   70,000 of  192,433 (2022-02-02 18:10:02.034738)\n",
      "Processed   80,000 of  192,433 (2022-02-02 18:11:22.875957)\n",
      "Processed   90,000 of  192,433 (2022-02-02 18:12:42.589241)\n",
      "Processed  100,000 of  192,433 (2022-02-02 18:14:00.350605)\n",
      "Processed  110,000 of  192,433 (2022-02-02 18:15:20.642944)\n",
      "Processed  120,000 of  192,433 (2022-02-02 18:16:39.921008)\n",
      "Processed  130,000 of  192,433 (2022-02-02 18:17:57.841917)\n",
      "Processed  140,000 of  192,433 (2022-02-02 18:19:14.589893)\n",
      "Processed  150,000 of  192,433 (2022-02-02 18:20:29.239788)\n",
      "Processed  160,000 of  192,433 (2022-02-02 18:21:46.607879)\n",
      "Processed  170,000 of  192,433 (2022-02-02 18:23:04.263851)\n",
      "Processed  180,000 of  192,433 (2022-02-02 18:24:20.817296)\n",
      "Processed  190,000 of  192,433 (2022-02-02 18:25:38.662329)\n",
      "10206929\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "def get_content(filename):\n",
    "    tree = etree.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    if root.find('.//HiddenText') is not None:\n",
    "        text = (root.find('.//HiddenText').text)\n",
    "\n",
    "    elif root.find('.//Text') is not None:\n",
    "        text = (root.find('.//Text').text)\n",
    "\n",
    "    else:\n",
    "        text = None\n",
    "    if text is None:\n",
    "        return ''\n",
    "    text = BeautifulSoup(text, parser='html.parser').get_text()\n",
    "    title = root.find('.//Title').text\n",
    "    return f'{title}. {text}'\n",
    "\n",
    "from collections import defaultdict\n",
    "writer = open('vocab/vocab.txt','w')\n",
    "freq = defaultdict(int)\n",
    "\n",
    "for idx,filename in enumerate(filenames):\n",
    "    if (idx) % 10000==0:\n",
    "        print(f'Processed {idx:8,} of {len(filenames):8,} ({datetime.datetime.now()})')\n",
    "    content = get_content(filename)\n",
    "    for token in nlp.tokenizer(content):\n",
    "        if not token.is_stop:\n",
    "            t = token.text.replace('\\n','')\n",
    "            freq[t]+=1\n",
    "            if freq[t]==2:\n",
    "                writer.write(f'{t}\\n')\n",
    "writer.close()\n",
    "print(len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "296af3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2171420\r\n"
     ]
    }
   ],
   "source": [
    "!cat vocab.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ae647fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed        0 of  192,433 (2022-02-02 19:52:17.316400)\n",
      "Processed   10,000 of  192,433 (2022-02-02 19:52:56.695617)\n",
      "Processed   20,000 of  192,433 (2022-02-02 19:53:36.739117)\n",
      "Processed   30,000 of  192,433 (2022-02-02 19:54:17.773373)\n",
      "Processed   40,000 of  192,433 (2022-02-02 19:54:56.308136)\n",
      "Processed   50,000 of  192,433 (2022-02-02 19:56:02.310448)\n",
      "Processed   60,000 of  192,433 (2022-02-02 19:57:14.398479)\n",
      "Processed   70,000 of  192,433 (2022-02-02 19:58:26.415487)\n",
      "Processed   80,000 of  192,433 (2022-02-02 19:59:40.181994)\n",
      "Processed   90,000 of  192,433 (2022-02-02 20:00:53.659325)\n",
      "Processed  100,000 of  192,433 (2022-02-02 20:02:07.289399)\n",
      "Processed  110,000 of  192,433 (2022-02-02 20:03:21.241653)\n",
      "Processed  120,000 of  192,433 (2022-02-02 20:04:35.424931)\n",
      "Processed  130,000 of  192,433 (2022-02-02 20:05:48.341121)\n",
      "Processed  140,000 of  192,433 (2022-02-02 20:07:00.799116)\n",
      "Processed  150,000 of  192,433 (2022-02-02 20:08:13.155880)\n",
      "Processed  160,000 of  192,433 (2022-02-02 20:09:27.024861)\n",
      "Processed  170,000 of  192,433 (2022-02-02 20:10:41.372548)\n",
      "Processed  180,000 of  192,433 (2022-02-02 20:11:53.475056)\n",
      "Processed  190,000 of  192,433 (2022-02-02 20:13:07.330226)\n",
      "1,486,315\n",
      "['glcncnlrn' 'caledon-' 'espert' 'firstever' '--taxi' 'shorthorns--'\n",
      " 'heretically' 'wyss' 'eurotrip' '-hkr' 'holkns' 'cirkinnoti' 'datent'\n",
      " '4:20,7:05,9:45' '100pr' '1112,900' 'r9v' 'sha-' 'ououav' 's450mo']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "freq = {}\n",
    "\n",
    "def filter_token(word):\n",
    "    word = word.replace('\\n','')\n",
    "    word = word.lower()\n",
    "    if not word.isnumeric() and len(''.join([char for char in word if not char in string.punctuation])): # LEN == QUE??\n",
    "        return word\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "for token in open('vocab.txt', 'r').read().splitlines():\n",
    "    token = filter_token(token)\n",
    "    if not token is None:\n",
    "        freq[token]=0\n",
    "\n",
    "for idx,filename in enumerate(filenames):\n",
    "    if (idx) % 10000==0:\n",
    "        print(f'Processed {idx:8,} of {len(filenames):8,} ({datetime.datetime.now()})')\n",
    "        \n",
    "    content = get_content(filename)\n",
    "    for token in nlp.tokenizer(content):\n",
    "        t = filter_token(token.text)\n",
    "        if t in freq:\n",
    "            freq[t]+=1\n",
    "\n",
    "print(f'{len(freq):,}')\n",
    "print(np.random.choice([word for word in freq],size=20,replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4ad2fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umbral:   0 - Cantidad de documentos en vocabulario:  1,486,315 (freq[word]>0)\n",
      "umbral:   1 - Cantidad de documentos en vocabulario:  1,486,315 (freq[word]>1)\n",
      "umbral:   2 - Cantidad de documentos en vocabulario:  1,002,528 (freq[word]>2)\n",
      "umbral:   3 - Cantidad de documentos en vocabulario:    760,010 (freq[word]>3)\n",
      "umbral:   4 - Cantidad de documentos en vocabulario:    619,974 (freq[word]>4)\n",
      "umbral:   5 - Cantidad de documentos en vocabulario:    529,571 (freq[word]>5)\n",
      "umbral:   6 - Cantidad de documentos en vocabulario:    466,150 (freq[word]>6)\n",
      "umbral:   7 - Cantidad de documentos en vocabulario:    419,431 (freq[word]>7)\n",
      "umbral:   8 - Cantidad de documentos en vocabulario:    383,317 (freq[word]>8)\n",
      "umbral:   9 - Cantidad de documentos en vocabulario:    354,064 (freq[word]>9)\n",
      "umbral:  10 - Cantidad de documentos en vocabulario:    330,101 (freq[word]>10)\n",
      "umbral:  11 - Cantidad de documentos en vocabulario:    310,310 (freq[word]>11)\n",
      "umbral:  12 - Cantidad de documentos en vocabulario:    293,127 (freq[word]>12)\n",
      "umbral:  13 - Cantidad de documentos en vocabulario:    278,116 (freq[word]>13)\n",
      "umbral:  14 - Cantidad de documentos en vocabulario:    265,257 (freq[word]>14)\n",
      "umbral:  15 - Cantidad de documentos en vocabulario:    253,749 (freq[word]>15)\n",
      "umbral:  16 - Cantidad de documentos en vocabulario:    243,627 (freq[word]>16)\n",
      "umbral:  17 - Cantidad de documentos en vocabulario:    234,527 (freq[word]>17)\n",
      "umbral:  18 - Cantidad de documentos en vocabulario:    226,481 (freq[word]>18)\n",
      "umbral:  19 - Cantidad de documentos en vocabulario:    218,779 (freq[word]>19)\n",
      "umbral: 192 - Cantidad de documentos en vocabulario:     53,509 (freq[word]>192)\n",
      "umbral: 1920 - Cantidad de documentos en vocabulario:     11,319 (freq[word]>1920)\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(20))+[192, 1920]:\n",
    "    print(f'umbral: {i:3} - Cantidad de documentos en vocabulario: {len([word for word in freq if freq[word]>i]):10,} (freq[word]>{i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd97c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd986056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toronto',\n",
       " 'new',\n",
       " 'said',\n",
       " 'room',\n",
       " 'canada',\n",
       " 'home',\n",
       " 'time',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'man',\n",
       " 'year',\n",
       " 'people',\n",
       " 'day',\n",
       " 'world',\n",
       " 'years',\n",
       " 'tho',\n",
       " 'city',\n",
       " 'work',\n",
       " 'street',\n",
       " 'wanted',\n",
       " 'business',\n",
       " 'york',\n",
       " 'house',\n",
       " 'phone',\n",
       " 'news',\n",
       " 'canadian',\n",
       " 'school',\n",
       " 'large',\n",
       " 'box',\n",
       " 'ing',\n",
       " 'west',\n",
       " 'ontario',\n",
       " 'ont',\n",
       " 'family',\n",
       " 'men',\n",
       " 'ltd',\n",
       " 'experience',\n",
       " 'old',\n",
       " 'globe',\n",
       " 'country',\n",
       " 'government',\n",
       " 'apply',\n",
       " 'war',\n",
       " 'sale',\n",
       " 'life',\n",
       " 'north',\n",
       " 'like',\n",
       " 'john',\n",
       " 'second',\n",
       " 'office']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_list = [(word, freq2[word]) for word in freq if not word.strip()=='' and len(word)>=3]\n",
    "word_frequency_list = sorted(word_frequency_list, key=lambda x: x[1],reverse=True)\n",
    "new_vocab = [word for word,_ in word_frequency_list[:50000]]\n",
    "new_vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18fe64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50,000'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{len(new_vocab):,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "238c287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = open('vocab/vocab_filtered.txt', 'w')\n",
    "writer.write('\\n'.join(new_vocab))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f34c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imm",
   "language": "python",
   "name": "imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
